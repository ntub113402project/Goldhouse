{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3ac482-437f-4a1c-bc56-26e5aa5e7a0c",
   "metadata": {},
   "source": [
    "# 比對描述有可能整段幾乎都是廣告 ex:16325563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c75df1-674a-4544-a0f9-1fa68357ff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\ckiptagger\\model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Items:\n",
      "HID1: 1, HID2: 16356457\n",
      "HID1: 1, HID2: 16325563\n",
      "HID1: 16356457, HID2: 16325563\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ckiptagger import WS #,POS,NER\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import re\n",
    "\n",
    "ws = WS(\"C:\\\\Users\\\\user\\\\OneDrive\\\\桌面\\\\AI_data\\\\data\")\n",
    "\"\"\"\n",
    "pos = POS(\"C:\\\\Users\\\\user\\\\OneDrive\\\\桌面\\\\AI_data\\\\data\")\n",
    "ner = NER(\"C:\\\\Users\\\\user\\\\OneDrive\\\\桌面\\\\AI_data\\\\data\")\n",
    "\"\"\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def preprocess_data(items):\n",
    "    for item in items:\n",
    "        address_text = ' '.join([str(a).strip() for a in item['positionround'].get('address', [])])\n",
    "        address_tokens = ws([address_text])\n",
    "        item['address_emb'] = get_bert_embedding(' '.join(address_tokens[0]))\n",
    "    return items\n",
    "\n",
    "def cosine_similarity(tensor1, tensor2):\n",
    "    return F.cosine_similarity(tensor1.unsqueeze(0), tensor2.unsqueeze(0)).item()\n",
    "\n",
    "def device_similarity(devices1, devices2):\n",
    "    common_devices = set(d1['device'] for d1 in devices1).intersection(set(d2['device'] for d2 in devices2))\n",
    "    matched = sum(1 for d1 in devices1 for d2 in devices2 if d1['device'] == d2['device'] and d1['avaliable'] == d2['avaliable'])\n",
    "    return matched / min(len(devices1), len(devices2)) >= 0.7\n",
    "\n",
    "#描述比較\n",
    "\"\"\"\n",
    "def clean_remark_content(content):\n",
    "    content = ' '.join(content) if isinstance(content, list) else content\n",
    "    words = ws([content])\n",
    "    pos_tags = pos(words)\n",
    "    entities = ner(words, pos_tags)\n",
    "    \n",
    "    clean_content = []\n",
    "    for word, pos_tag, entity in zip(words[0], pos_tags[0], entities[0]):\n",
    "        if pos_tag in {'Nc'} or entity[1] in {'LOC'}: #地點名詞、地理位置\n",
    "            clean_content.append(word)\n",
    "    \n",
    "    cleaned_text = ' '.join(clean_content)\n",
    "    cleaned_text = re.sub(r'\\b(?:\\d{1,3}[-.\\s]?){3,4}\\d{1,4}\\b', '', cleaned_text) \n",
    "    cleaned_text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', cleaned_text)  \n",
    "    cleaned_text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', cleaned_text)  \n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)  \n",
    "    return cleaned_text\n",
    "    \n",
    "#描述比較的閥值設定\n",
    "def is_description_similar(desc1, desc2):\n",
    "    emb1 = get_bert_embedding(desc1)\n",
    "    emb2 = get_bert_embedding(desc2)\n",
    "    similarity = cosine_similarity(emb1, emb2)\n",
    "    return similarity >= 0.6\n",
    "\"\"\"\n",
    "def compare_patterns(pattern1, pattern2):\n",
    "    return (len(set(pattern1.split()).intersection(pattern2.split())) / min(len(pattern1.split()), len(pattern2.split()))) > 0.9\n",
    "\n",
    "def compare_layers(layer1, layer2):\n",
    "    layers1 = set(layer1.split('/'))\n",
    "    layers2 = set(layer2.split('/'))\n",
    "    return bool(layers1.intersection(layers2))\n",
    "\n",
    "def find_similar_items(data):\n",
    "    similar_items = []\n",
    "    n = len(data)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            address_similarity = cosine_similarity(data[i]['address_emb'], data[j]['address_emb']) > 0.9\n",
    "            if address_similarity:\n",
    "                pattern_match = compare_patterns(data[i]['houseinfo']['pattern'], data[j]['houseinfo']['pattern'])\n",
    "                size_match = compare_patterns(data[i]['houseinfo']['size'], data[j]['houseinfo']['size'])\n",
    "                layer_match = compare_layers(data[i]['houseinfo']['layer'], data[j]['houseinfo']['layer'])\n",
    "                device_match = device_similarity(data[i]['servicelist'], data[j]['servicelist'])\n",
    "                #description_match=is_description_similar(clean_remark_content(data[i]['remark']['content']), clean_remark_content(data[j]['remark']['content']))\n",
    "                if pattern_match and size_match and layer_match and device_match:# and description_match:\n",
    "                    similar_items.append((data[i]['hid'], data[j]['hid']))\n",
    "    return similar_items\n",
    "\n",
    "def main():\n",
    "    json_data = load_json(\"C:\\\\Users\\\\user\\\\OneDrive\\\\桌面\\\\detail.json\")\n",
    "    complete_data = preprocess_data(json_data)\n",
    "    similar_items = find_similar_items(complete_data)\n",
    "    print(\"Similar Items:\")\n",
    "    for hid1, hid2 in similar_items[:3]:  \n",
    "        print(f\"HID1: {hid1}, HID2: {hid2}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12fa76-9798-4195-bd2b-c66fef050171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
